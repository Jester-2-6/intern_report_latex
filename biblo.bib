
@article{trailnet,
	Archiveprefix = {arXiv},
	Author = {Nikolai Smolyanskiy and Alexey Kamenev and Jeffrey Smith and Stan Birchfield},
	Title = {Toward Low-Flying Autonomous MAV Trail Navigation using Deep Neural Networks for Environmental Awareness},
	Url = {https://arxiv.org/pdf/1705.02550.pdf},
	Year = {2017}
	}

@article{resnet,
	Archiveprefix = {arXiv},
	Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/HeZRS15},
	Eprint = {1512.03385},
	Journal = {CoRR},
	Timestamp = {Wed, 07 Jun 2017 14:41:17 +0200},
	Title = {Deep Residual Learning for Image Recognition},
	Url = {http://arxiv.org/abs/1512.03385},
	Volume = {abs/1512.03385},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1512.03385}}

@misc{jetson_tx2,
	Howpublished = {\url{https://developer.nvidia.com/embedded/buy/jetson-tx2}},
	Note = {{A}ccessed: 2018-06-28},
	Title = {NVIDIA Jetson TX2 Module}}

@article{yolo,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	shorttitle = {You {Only} {Look} {Once}},
	url = {http://arxiv.org/abs/1506.02640},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
	urldate = {2018-12-31},
	journal = {arXiv:1506.02640 [cs]},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02640},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1506.02640 PDF:C\:\\Users\\abara\\Zotero\\storage\\Z3HHNFPL\\Redmon et al. - 2015 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\abara\\Zotero\\storage\\3BJ3NMPP\\1506.html:text/html}
}

@misc{presentation,
	title = {Presentation: {ML} {Pipeline} - {DATA}61 {RRG}},
	Howpublished = {\url{https://docs.google.com/presentation/d/1Z7OW8ILDy-wdoWo1Hw84-F0VBjzLAXzpfrpAYbkIddE}}
	}



@misc{tfrecords,
	title = {Using {TFRecords} and tf.{Example}},
	Howpublished = {\url{https://www.tensorflow.org/tutorials/load_data/tf-records}},
	language = {en},
	urldate = {2018-12-31},
	journal = {TensorFlow},
	file = {Snapshot:C\:\\Users\\abara\\Zotero\\storage\\5869JMR9\\tf-records.html:text/html}
}

@misc{darpa,
	title = {{DARPA} {Subterranean} {Challenge}: {Unearthing} the {Subterranean} {Environment}},
	shorttitle = {{DARPA} {Subterranean} {Challenge}},
	Howpublished = {\url{https://subtchallenge.com/}},
	abstract = {The DARPA Subterranean Challenge aims to explore new approaches to rapidly map, navigate, and search underground environments.},
	language = {en},
	urldate = {2018-12-31},
	journal = {DARPA Subterranean Challenge},
	author = {team, Booz Allen DX},
	file = {Snapshot:C\:\\Users\\abara\\Zotero\\storage\\5PLWWDNS\\subtchallenge.com.html:text/html}
}

@article{trailnet2,
	title = {A {Machine} {Learning} {Approach} to {Visual} {Perception} of {Forest} {Trails} for {Mobile} {Robots}},
	volume = {1},
	issn = {2377-3766},
	doi = {10.1109/LRA.2015.2509024},
	abstract = {We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Giusti, A. and Guzzi, J. and Cireşan, D. C. and He, F. and Rodríguez, J. P. and Fontana, F. and Faessler, M. and Forster, C. and Schmidhuber, J. and Caro, G. D. and Scaramuzza, D. and Gambardella, L. M.},
	month = jul,
	year = {2016},
	keywords = {Aerial Robotics, autonomous aerial vehicles, Cameras, Deep Learning, deep-neural network, forest trails, helicopters, image classification, Image segmentation, learning (artificial intelligence), Machine Learning, machine learning approach, microrobots, mobile robots, Mobile robots, monocular image, neural nets, quadrotor microaerial vehicle control, qualitative analysis, quantitative analysis, Roads, robot vision, Robot vision systems, supervised image classifier, viewing direction, visual perception, Visual perception, Visual-Based Navigation},
	pages = {661--667},
	file = {Full Text:C\:\\Users\\abara\\Zotero\\storage\\3CMXZB3L\\Giusti et al. - 2016 - A Machine Learning Approach to Visual Perception o.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\abara\\Zotero\\storage\\2G3VSI4B\\7358076.html:text/html}
}

@article{trailnet3,
	title = {An {Optimised} {Deep} {Neural} {Network} {Approach} for {Forest} {Trail} {Navigation} for {UAV} {Operation} within the {Forest} {Canopy}},
	abstract = {Autonomous ﬂight within a forest canopy represents a key challenge for generalised scene understanding on-board a future Unmanned Aerial Vehicle (UAV) platform. Here we present an approach for automatic trail navigation within such an environment that successfully generalises across diﬀering image resolutions allowing UAV with varying sensor payload capabilities to operate equally in such challenging environmental conditions. Speciﬁcally, this work presents an optimised deep neural network architecture, capable of stateof-the-art performance across varying resolution aerial UAV imagery, that improves forest trail detection for UAV guidance even when using signiﬁcantly low resolution images that are representative of low-cost search and rescue capable UAV platforms.},
	language = {en},
	author = {Maciel-Pearson, B G and Breckon, T P},
	pages = {3},
	file = {Maciel-Pearson and Breckon - An Optimised Deep Neural Network Approach for Fore.pdf:C\:\\Users\\abara\\Zotero\\storage\\XLCM9FHM\\Maciel-Pearson and Breckon - An Optimised Deep Neural Network Approach for Fore.pdf:application/pdf}
}

@inproceedings{trailnet4,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Extending {Deep} {Neural} {Network} {Trail} {Navigation} for {Unmanned} {Aerial} {Vehicle} {Operation} {Within} the {Forest} {Canopy}},
	isbn = {978-3-319-96728-8},
	abstract = {Autonomous flight within a forest canopy represents a key challenge for generalised scene understanding on-board a future Unmanned Aerial Vehicle (UAV) platforms. Here we present an approach for automatic trail navigation within such an unstructured environment that successfully generalises across differing image resolutions - allowing UAV with varying sensor payload capabilities to operate equally in such challenging environmental conditions. Specifically, this work presents an optimised deep neural network architecture, capable of state-of-the-art performance across varying resolution aerial UAV imagery, that improves forest trail detection for UAV guidance even when using significantly low resolution images that are representative of low-cost search and rescue capable UAV platforms.},
	language = {en},
	booktitle = {Towards {Autonomous} {Robotic} {Systems}},
	publisher = {Springer International Publishing},
	author = {Maciel-Pearson, Bruna G. and Carbonneau, Patrice and Breckon, Toby P.},
	editor = {Giuliani, Manuel and Assaf, Tareq and Giannaccini, Maria Elena},
	year = {2018},
	keywords = {Autonomous UAV, Deep learning, Trail detection, Unstructured environment},
	pages = {147--158}
}

@misc{idsia,
	title = {IDISIA Dataset},
	Howpublished = {\url{http://people.idsia.ch/~guzzi/DataSet.html}},
	urldate = {2019-01-04},
	file = {:C\:\\Users\\abara\\Zotero\\storage\\WJ2NEJ9J\\DataSet.html:text/html}
}

@article{hand_eye,
	title = {Learning {Hand}-{Eye} {Coordination} for {Robotic} {Grasping} with {Deep} {Learning} and {Large}-{Scale} {Data} {Collection}},
	url = {http://arxiv.org/abs/1603.02199},
	abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
	urldate = {2019-01-04},
	journal = {arXiv:1603.02199 [cs]},
	author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.02199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: This is an extended version of "Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection," ISER 2016. Draft modified to correct typo in Algorithm 1 and add a link to the publicly available dataset}
}

@misc{top25ai,
	title = {Wave {Computing} named a top 25 {AI} solution Provider},
	Howpublished = {\url{https://globenewswire.com/news-release/2017/07/20/1054678/0/en/Wave-Computing-Named-a-Top-25-AI-Solution-Provider-for-2017.html}},
	urldate = {2019-01-08}
}

@misc{waveintro,
	title = {About {Wave Computing}},
	Howpublished = {\url{https://wavecomp.ai/company}},
	urldate = {2019-01-08}
}

@misc{tflow,
	title = {About {TensorFlow}},
	Howpublished = {\url{https://www.tensorflow.org/?hl=hi}},
	urldate = {2019-01-08}
}

@misc{pqmintro,
	title = {About {Paraqum Technologies}},
	Howpublished = {\url{https://paraqum.com/about}},
	urldate = {2019-01-08}
}

@misc{mipsaq,
	title = {MIPS bought by Wave Computing},
	Howpublished = {\url{https://www.electronicsweekly.com/news/business/mips-bought-wave-computing-2018-06/}},
	urldate = {2019-01-08}
}